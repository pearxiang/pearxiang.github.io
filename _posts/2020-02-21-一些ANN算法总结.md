# ANN算法
ANN搜索：approximate nearest neighbor, 本质上是在很多稠密向量中，迅速找到目标点的临近点，并认为这认为是相似的节点，主要用于图像检索、高维检索。几乎所有的方法都是对空间进行切分，可以迅速找到子空间，并与子空间的数据进行计算。方法主要有基于树的方法、哈希方法、矢量量化、基于图的方法。

## 1. 基于树的方法
主要就是利用树的数据结构来表达对空间的划分，KD树是最经典的例子
### 1.1 KD树
![KD-Tree例子](https://github.com/pearxiang/wukong/blob/master/resources/images/KD-TREE%E4%BE%8B%E5%AD%90.png?raw=true)

KD树构建的过程就是迭代二分空间的过程
**经典算法：选择方差最大的维度,计算中位数点，作为划分点，分为左右子树，迭代上述过程, 直到空间上的点小于阈值**
***选择方差最大的维度，便于切分空间，使得整个树的深度最小***

**随机KD树算法：区别与经典算法，主要是随机选择一个维度来切分，最终构造m个树(横看成岭侧成峰,不同角度来看空间点的树分布)**

### 1.2 annoy
不断的用选取的两个质心的法平面对空间切分，最终将子空间的数据点控制在K以内，对于待插入样本X，从根节点ROOT使用法向量与X做内积运算判断在左子树还是右子树。构建多个树提高查询召回率

## 2. 哈希方法（局部敏感性哈希）
局部敏感：相近的样本点比相远的点更容易碰撞
加速查找：查找目标分在哪个桶中，只需要在桶中遍历比较
多表哈希：当哈希函数数目K取得太大，查询样本与其对应的最近邻分配到同一个桶的概率很小，建立多个表，增加召回率
LSH主要涉及到三个参数
K：每一个哈希表的哈希函数数目（空间划分）
L：哈希表的数目
T：近邻哈希桶的数目

## 3. 矢量量化方法
**将一个向量空间中的点用其中的一个有限子集来编码的过程**
## 4. 基于图的方法
### 4.1 NSW
### 4.2 HNSW
**分层的可导航小世界(Hierarchical Navigable Small Word), NSW基础上引入跳表概念，加快搜索跳转速度**

***选边策略1: 节点q最近的N个邻居,按照距离选出M个作为邻居进行连接即可***

***选边策略2: 节点q最近的N个邻居，选出最近的节点，加入邻居队列，然后剩余的节点选出a进行判断，如果q到到a的距离大于所有的邻居节点到a的距离，则将q->a进行连接，因为q不能通过邻居节点更接近a,所以直接将q和a连接,建立一条高速公路***


# ANN算法的局限

需要对两者向量进行度量计算，一般就是cosine计算或者L2距离，计算函数较为单一，在搜索上和推荐上，不能完全试用，需要二者向量空间一致，反映到计算过程上，就是需要两个平行空间的映射关系，将不同空间的点映射到一起（见图a）
![神经网络向量匹配训练模型](https://github.com/pearxiang/wukong/blob/master/resources/images/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B.jpg?raw=true)

先表征计算，然后在做简单的向量匹配可能无法表达部分复杂的非线性计算，如一些深度模型的，实际例子中，譬如一个用户，可能喜欢两部类型差异完全不同的影片，那么不同差异很大的item很难映射到一起，而图b则是会对向量过神经网络复杂计算表征一些非线性很复杂的关系。图b这种向量匹配形式就很难在上述的KD-TREE, LSH方法中取得良好的效果，而基于图的方法经过改良是可以去完成的，甚至能计算表征维度不同向量的关系远近。   
